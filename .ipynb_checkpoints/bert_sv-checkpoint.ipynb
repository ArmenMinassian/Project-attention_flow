{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dehghani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from attention_graph_util import *\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "import matplotlib as mpl\n",
    "import networkx as nx\n",
    "import os\n",
    "from util import constants\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import pandas as pd\n",
    "\n",
    "from util.models import MODELS\n",
    "from util.tasks import TASKS\n",
    "#from dnotebook_utils import *\n",
    "from attention_graph_util import *\n",
    "%matplotlib inline\n",
    "from util.config_util import get_task_params\n",
    "from notebooks.notebook_utils import *\n",
    "from util import inflect\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "import math\n",
    "\n",
    "\n",
    "rc={'font.size': 10, 'axes.labelsize': 10, 'legend.fontsize': 10.0, \n",
    "    'axes.titlesize': 32, 'xtick.labelsize': 20, 'ytick.labelsize': 16}\n",
    "plt.rcParams.update(**rc)\n",
    "mpl.rcParams['axes.linewidth'] = .5 #set the value globally\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: transformers in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (1.12.43)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers==0.5.2 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (2020.4.4)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (0.0.41)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (4.43.0)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.43 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from boto3->transformers) (1.15.43)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: networkx in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (2.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from networkx) (4.4.2)\n",
      "Requirement already up-to-date: matplotlib in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Requirement already up-to-date: seaborn in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from seaborn) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.0.1 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=2.1.2 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from seaborn) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=0.22.0 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from seaborn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib>=2.1.2->seaborn) (1.14.0)\n",
      "Requirement already satisfied: torch in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (1.5.0)\n",
      "Requirement already satisfied: torchvision in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from torch) (1.18.1)\n",
      "Requirement already satisfied: future in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/dehghani/anaconda3/envs/indist/lib/python3.7/site-packages (from torchvision) (7.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install networkx\n",
    "!pip install --upgrade matplotlib\n",
    "!pip install --upgrade seaborn\n",
    "\n",
    "\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab len:  10032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Constructing tf.data.Dataset for split validation, from ../InDist/data/word_sv_agreement/0.1.0\n",
      "INFO:absl:Constructing tf.data.Dataset for split test, from ../InDist/data/word_sv_agreement/0.1.0\n",
      "INFO:absl:Constructing tf.data.Dataset for split train, from ../InDist/data/word_sv_agreement/0.1.0\n"
     ]
    }
   ],
   "source": [
    "task_name = 'word_sv_agreement_lm'\n",
    "task_params = get_task_params(batch_size=1)\n",
    "task = TASKS[task_name](task_params, data_dir='../InDist/data')\n",
    "cl_token = task.sentence_encoder().encode(constants.bos)\n",
    "task_tokenizer = task.sentence_encoder()._tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased',\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_convertor(encoded_input_task, task_offset, task_encoder, tokenizer):\n",
    "    string_part1 = task_encoder.decode(encoded_input_task[:task_offset])\n",
    "    tokens_part1 = tokenizer.tokenize(string_part1)\n",
    "    \n",
    "    return len(tokens_part1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many NNS of woodland remain and support a JJ sector in the southern portion of the state .\n",
      "21 ['cls', 'many', 'n', '##ns', 'of', 'woodland', 'remain', 'and', 'support', 'a', 'jj', 'sector', 'in', 'the', 'southern', 'portion', 'of', 'the', 'state', '.', 'sep']\n",
      "torch.Size([1, 21, 30522])\n",
      "(6, 12, 21, 21)\n",
      "torch.Size([1, 21, 768])\n",
      "tensor(-3811819.5000, grad_fn=<SumBackward0>)\n",
      "torch.Size([1, 21, 768])\n"
     ]
    }
   ],
   "source": [
    "for x,y in task.test_dataset:\n",
    "    sentence = task.sentence_encoder().decode(x[0][1:])\n",
    "    print(sentence)\n",
    "    break\n",
    "\n",
    "tokens = ['cls']+tokenizer.tokenize(sentence)+['sep']\n",
    "print(len(tokens), tokens)\n",
    "tf_input_ids = tokenizer.encode(sentence)\n",
    "input_ids = torch.tensor([tf_input_ids])\n",
    "logits, all_hidden_states, all_attentions = model(input_ids)\n",
    "print(logits.shape)\n",
    "_attentions = [att.detach().numpy() for att in all_attentions]\n",
    "attentions_mat = np.asarray(_attentions)[:,0]\n",
    "print(attentions_mat.shape)\n",
    "\n",
    "embeded_inputs = torch.autograd.Variable(model.distilbert.embeddings(input_ids), requires_grad=True)\n",
    "logits, all_hidden_states, all_attentions = model(inputs_embeds=embeded_inputs)\n",
    "print(embeded_inputs.shape)\n",
    "\n",
    "\n",
    "lsum = logits.sum()\n",
    "print(lsum)\n",
    "\n",
    "lsum.backward()\n",
    "embeded_inputs.require_grad = True\n",
    "print(embeded_inputs.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Constructing tf.data.Dataset for split validation, from ../InDist/data/word_sv_agreement/0.1.0\n",
      "999it [05:59,  3.04it/s]"
     ]
    }
   ],
   "source": [
    "all_examples_x = []\n",
    "all_examples_y = []\n",
    "all_examples_attentions = []\n",
    "all_examples_blankout_relevance = []\n",
    "all_examples_grads = []\n",
    "all_examples_inputgrads = []\n",
    "n_batches = 10\n",
    "\n",
    "all_examples_accuracies = []\n",
    "\n",
    "infl_eng = inflect.engine()\n",
    "verb_infl, noun_infl = gen_inflect_from_vocab(infl_eng, '../InDist/notebooks/wiki.vocab')\n",
    "\n",
    "test_data = task.databuilder.as_dataset(split='validation', batch_size=1)\n",
    "for examples in tqdm(test_data):\n",
    "    sentence = task.sentence_encoder().decode(examples['sentence'][0])\n",
    "    \n",
    "    verb_position = examples['verb_position'][0].numpy()+1  #+1 because of adding cls.\n",
    "    verb_position = offset_convertor(examples['sentence'][0], verb_position, task.sentence_encoder(), tokenizer)\n",
    "    \n",
    "    sentence = ['cls']+tokenizer.tokenize(sentence)+['sep']\n",
    "    \n",
    "    \n",
    "    sentence[verb_position] = tokenizer.mask_token\n",
    "    tf_input_ids = tokenizer.encode(sentence)\n",
    "    input_ids = torch.tensor([tf_input_ids])\n",
    "    \n",
    "\n",
    "    \n",
    "    s_shape = input_ids.shape\n",
    "    batch_size, length = s_shape[0], s_shape[1]\n",
    "    actual_verb = examples['verb'][0].numpy().decode(\"utf-8\")\n",
    "    inflected_verb = verb_infl[actual_verb] \n",
    "\n",
    "\n",
    "    actual_verb_index = tokenizer.encode(tokenizer.tokenize(actual_verb))[1]\n",
    "    inflected_verb_index = tokenizer.encode(tokenizer.tokenize(inflected_verb))[1]\n",
    "\n",
    "    all_examples_x.append(input_ids)\n",
    "    embeded_inputs = torch.autograd.Variable(model.distilbert.embeddings(input_ids), requires_grad=True)\n",
    "    predictions = model(inputs_embeds=embeded_inputs)\n",
    "    logits = predictions[0][0]\n",
    "\n",
    "    \n",
    "        \n",
    "    probs = torch.nn.Softmax(dim=-1)(logits)\n",
    "    actual_verb_score = probs[verb_position][actual_verb_index]\n",
    "    inflected_verb_score = probs[verb_position][inflected_verb_index]\n",
    "    \n",
    "    main_diff_score = actual_verb_score - inflected_verb_score\n",
    "    \n",
    "    all_examples_accuracies.append(main_diff_score > 0)\n",
    "    \n",
    "    main_diff_score.backward()\n",
    "    grads = embeded_inputs.grad\n",
    "    grad_scores = abs(np.sum(grads.detach().numpy(), axis=-1))\n",
    "    input_grad_scores = abs(np.sum((grads * embeded_inputs).detach().numpy(), axis=-1))\n",
    "    all_examples_grads.append(grad_scores)\n",
    "    all_examples_inputgrads.append(input_grad_scores)\n",
    "    \n",
    "    hidden_states, attentions = predictions[-2:]\n",
    "    _attentions = [att.detach().numpy() for att in attentions]\n",
    "    attentions_mat = np.asarray(_attentions)[:,0]\n",
    "\n",
    "    all_examples_attentions.append(attentions_mat)\n",
    "    \n",
    "    # Repeating examples and replacing one token at a time with unk\n",
    "    batch_size = 1\n",
    "    max_len = input_ids.shape[1]\n",
    "    \n",
    "    # Repeat each example 'max_len' times\n",
    "    x = input_ids\n",
    "    extended_x = np.reshape(np.tile(x[:,None,...], (1, max_len, 1)),(-1,x.shape[-1]))\n",
    "\n",
    "    # Create unk sequences and unk mask\n",
    "    unktoken = tokenizer.encode([tokenizer.mask_token])[1]\n",
    "    unks = unktoken * np.eye(max_len)\n",
    "    unks =  np.tile(unks, (batch_size, 1))\n",
    "    \n",
    "    unk_mask =  (unktoken - unks)/unktoken\n",
    "  \n",
    "    # Replace one token in each repeatition with unk\n",
    "    extended_x = extended_x * unk_mask + unks\n",
    "    \n",
    "    # Get the new output\n",
    "    extended_predictions = model(torch.tensor(extended_x, dtype=torch.int64))\n",
    "    extended_logits = extended_predictions[0]\n",
    "    extended_probs = torch.nn.Softmax(dim=-1)(extended_logits)\n",
    "    \n",
    "    extended_correct_probs = extended_probs[:,verb_position,actual_verb_index]\n",
    "    extended_wrong_probs =  extended_probs[:,verb_position,inflected_verb_index]\n",
    "    extended_diff_scores = extended_correct_probs - extended_wrong_probs\n",
    "    \n",
    "    # Save the difference in the probability predicted for the correct class\n",
    "    diffs = abs(main_diff_score - extended_diff_scores)\n",
    "\n",
    "    all_examples_blankout_relevance.append(diffs.detach())\n",
    "    n_batches -= 1\n",
    "    if n_batches <= 0:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_att_relevance(full_att_mat, input_tokens, layer=-1):\n",
    "    cls_index = 0\n",
    "    raw_rel = full_att_mat[layer].sum(axis=0)[cls_index]/full_att_mat[layer].sum(axis=0)[cls_index].sum()\n",
    "    \n",
    "    return raw_rel\n",
    "\n",
    "\n",
    "def get_joint_relevance(full_att_mat, input_tokens, layer=-1, output_index=0):\n",
    "    att_sum_heads =  full_att_mat.sum(axis=1)/8\n",
    "    joint_attentions = compute_joint_attention(att_sum_heads, add_residual=True)\n",
    "    relevance_attentions = joint_attentions[layer][output_index]\n",
    "    return relevance_attentions\n",
    "\n",
    "\n",
    "def get_flow_relevance(full_att_mat, input_tokens, layer):\n",
    "    \n",
    "    input_tokens = input_tokens\n",
    "    res_att_mat = full_att_mat.sum(axis=1)/full_att_mat.shape[1]\n",
    "    res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...]\n",
    "    res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None]\n",
    "\n",
    "    res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=input_tokens)\n",
    "    \n",
    "    A = res_adj_mat\n",
    "    res_G=nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n",
    "    for i in np.arange(A.shape[0]):\n",
    "        for j in np.arange(A.shape[1]):\n",
    "            nx.set_edge_attributes(res_G, {(i,j): A[i,j]}, 'capacity')\n",
    "\n",
    "\n",
    "    output_nodes = ['L'+str(layer+1)+'_0']\n",
    "    input_nodes = []\n",
    "    for key in res_labels_to_index:\n",
    "        if res_labels_to_index[key] < full_att_mat.shape[-1]:\n",
    "            input_nodes.append(key)\n",
    "    \n",
    "    flow_values = compute_node_flow(res_G, res_labels_to_index, input_nodes, output_nodes, length=full_att_mat.shape[-1])\n",
    "    \n",
    "    n_layers = full_att_mat.shape[0]\n",
    "    length = full_att_mat.shape[-1]\n",
    "    final_layer_attention = flow_values[(layer+1)*length:,layer*length:(layer+1)*length]\n",
    "    cls_index = 0\n",
    "    relevance_attention_raw = final_layer_attention[cls_index]\n",
    "\n",
    "    return relevance_attention_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 14470.55it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15242.09it/s]\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute raw relevance scores ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 15334.88it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15259.45it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15390.02it/s]\n",
      "\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 15335.44it/s]\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      " 42%|████▎     | 425/1000 [00:00<00:00, 4247.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute joint relevance scores ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4130.59it/s][A\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      " 46%|████▌     | 457/1000 [00:00<00:00, 4569.08it/s]\u001b[A\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4539.99it/s][A\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      " 46%|████▋     | 464/1000 [00:00<00:00, 4637.15it/s]\u001b[A\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4584.75it/s][A\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      " 47%|████▋     | 466/1000 [00:00<00:00, 4641.51it/s]\u001b[A\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4590.57it/s][A\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      " 46%|████▋     | 464/1000 [00:00<00:00, 4631.09it/s]\u001b[A\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4589.91it/s][A\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      " 46%|████▋     | 464/1000 [00:00<00:00, 4604.48it/s]\u001b[A\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4534.28it/s][A\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute flow relevance scores ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/1000 [00:00<12:57,  1.28it/s]\u001b[A\n",
      "  0%|          | 2/1000 [00:05<34:17,  2.06s/it]\u001b[A\n",
      "  0%|          | 4/1000 [00:06<24:54,  1.50s/it]\u001b[A\n",
      "  0%|          | 5/1000 [00:06<20:05,  1.21s/it]\u001b[A\n",
      "  1%|          | 6/1000 [00:15<56:23,  3.40s/it]\u001b[A\n",
      "  1%|          | 7/1000 [00:16<47:20,  2.86s/it]\u001b[A\n",
      "  1%|          | 8/1000 [00:17<36:13,  2.19s/it]\u001b[A\n",
      "  1%|          | 10/1000 [00:18<27:22,  1.66s/it]\u001b[A\n",
      "  1%|          | 11/1000 [00:21<35:10,  2.13s/it]\u001b[A\n",
      "  1%|          | 12/1000 [00:21<25:23,  1.54s/it]\u001b[A\n",
      "  1%|▏         | 13/1000 [00:21<18:49,  1.14s/it]\u001b[A\n",
      "  1%|▏         | 14/1000 [00:22<18:14,  1.11s/it]\u001b[A\n",
      "  2%|▏         | 15/1000 [00:26<28:49,  1.76s/it]\u001b[A\n",
      "  2%|▏         | 16/1000 [00:28<33:01,  2.01s/it]\u001b[A\n",
      "  2%|▏         | 17/1000 [00:28<23:40,  1.45s/it]\u001b[A\n",
      "  2%|▏         | 18/1000 [00:32<34:57,  2.14s/it]\u001b[A\n",
      "  2%|▏         | 19/1000 [00:32<25:30,  1.56s/it]\u001b[A\n",
      "  2%|▏         | 21/1000 [00:33<19:22,  1.19s/it]\u001b[A\n",
      "  2%|▏         | 22/1000 [00:34<18:12,  1.12s/it]\u001b[A\n",
      "  2%|▏         | 24/1000 [00:39<25:15,  1.55s/it]\u001b[A\n",
      "  2%|▎         | 25/1000 [00:47<56:37,  3.49s/it]\u001b[A\n",
      "  3%|▎         | 27/1000 [00:50<47:04,  2.90s/it]\u001b[A\n",
      "  3%|▎         | 28/1000 [00:52<42:55,  2.65s/it]\u001b[A\n",
      "  3%|▎         | 29/1000 [00:55<40:41,  2.51s/it]\u001b[A\n",
      "  3%|▎         | 31/1000 [01:03<47:55,  2.97s/it]\u001b[A\n",
      "  3%|▎         | 33/1000 [01:04<36:13,  2.25s/it]\u001b[A\n",
      "  3%|▎         | 34/1000 [01:09<49:48,  3.09s/it]\u001b[A\n",
      "  4%|▎         | 35/1000 [01:09<35:43,  2.22s/it]\u001b[A\n",
      "  4%|▎         | 36/1000 [01:15<56:24,  3.51s/it]\u001b[A\n",
      "  4%|▎         | 37/1000 [01:16<40:00,  2.49s/it]\u001b[A\n",
      "  4%|▍         | 38/1000 [01:17<35:08,  2.19s/it]\u001b[A\n",
      "  4%|▍         | 39/1000 [01:19<35:32,  2.22s/it]\u001b[A\n",
      "  4%|▍         | 40/1000 [01:19<25:24,  1.59s/it]\u001b[A\n",
      "  4%|▍         | 41/1000 [01:22<29:01,  1.82s/it]\u001b[A\n",
      "  4%|▍         | 42/1000 [01:23<25:52,  1.62s/it]\u001b[A\n",
      "  4%|▍         | 43/1000 [01:27<38:36,  2.42s/it]\u001b[A\n",
      "  4%|▍         | 44/1000 [01:39<1:21:27,  5.11s/it]\u001b[A\n",
      "  5%|▍         | 46/1000 [01:39<58:00,  3.65s/it]  \u001b[A\n",
      "  5%|▍         | 47/1000 [01:39<41:19,  2.60s/it]\u001b[A\n",
      "  5%|▍         | 48/1000 [01:40<29:55,  1.89s/it]\u001b[A\n",
      "  5%|▍         | 49/1000 [01:40<21:56,  1.38s/it]\u001b[A\n",
      "  5%|▌         | 50/1000 [01:42<24:03,  1.52s/it]\u001b[A\n",
      "  5%|▌         | 51/1000 [01:42<19:45,  1.25s/it]\u001b[A\n",
      "  5%|▌         | 52/1000 [01:42<14:21,  1.10it/s]\u001b[A\n",
      "  5%|▌         | 53/1000 [01:43<12:59,  1.21it/s]\u001b[A\n",
      "  6%|▌         | 55/1000 [01:45<15:01,  1.05it/s]\u001b[A\n",
      "  6%|▌         | 56/1000 [01:46<13:16,  1.19it/s]\u001b[A\n",
      "  6%|▌         | 57/1000 [01:47<15:09,  1.04it/s]\u001b[A\n",
      "  6%|▌         | 58/1000 [01:48<13:06,  1.20it/s]\u001b[A\n",
      "  6%|▌         | 59/1000 [01:48<10:10,  1.54it/s]\u001b[A\n",
      "  6%|▌         | 60/1000 [01:54<34:34,  2.21s/it]\u001b[A\n",
      "  6%|▌         | 61/1000 [01:57<37:25,  2.39s/it]\u001b[A\n",
      "  6%|▌         | 62/1000 [01:57<28:06,  1.80s/it]\u001b[A\n",
      "  6%|▋         | 63/1000 [01:58<24:28,  1.57s/it]\u001b[A\n",
      "  6%|▋         | 65/1000 [02:01<23:40,  1.52s/it]\u001b[A\n",
      "  7%|▋         | 66/1000 [02:02<21:58,  1.41s/it]\u001b[A\n",
      "  7%|▋         | 68/1000 [02:03<17:37,  1.13s/it]\u001b[A\n",
      "  7%|▋         | 69/1000 [02:03<12:51,  1.21it/s]\u001b[A\n",
      "  7%|▋         | 70/1000 [02:06<22:20,  1.44s/it]\u001b[A\n",
      "  7%|▋         | 71/1000 [02:08<26:56,  1.74s/it]\u001b[A\n",
      "  7%|▋         | 72/1000 [02:09<22:15,  1.44s/it]\u001b[A\n",
      "  7%|▋         | 73/1000 [02:10<17:36,  1.14s/it]\u001b[A\n",
      "  8%|▊         | 75/1000 [02:10<13:11,  1.17it/s]\u001b[A\n",
      "  8%|▊         | 76/1000 [02:13<20:33,  1.33s/it]\u001b[A\n",
      "  8%|▊         | 77/1000 [02:13<18:44,  1.22s/it]\u001b[A\n",
      "  8%|▊         | 78/1000 [02:14<16:28,  1.07s/it]\u001b[A\n",
      "  8%|▊         | 79/1000 [02:14<12:30,  1.23it/s]\u001b[A\n",
      "  8%|▊         | 81/1000 [02:16<12:33,  1.22it/s]\u001b[A\n",
      "  8%|▊         | 82/1000 [02:16<10:07,  1.51it/s]\u001b[A\n",
      "  8%|▊         | 83/1000 [02:22<31:10,  2.04s/it]\u001b[A\n",
      "  8%|▊         | 84/1000 [02:23<28:38,  1.88s/it]\u001b[A\n",
      "  8%|▊         | 85/1000 [02:24<22:40,  1.49s/it]\u001b[A\n",
      "  9%|▊         | 86/1000 [02:25<19:56,  1.31s/it]\u001b[A\n",
      "  9%|▊         | 87/1000 [02:25<15:14,  1.00s/it]\u001b[A\n",
      "  9%|▉         | 88/1000 [02:25<12:52,  1.18it/s]\u001b[A\n",
      "  9%|▉         | 89/1000 [02:31<33:43,  2.22s/it]\u001b[A\n",
      "  9%|▉         | 91/1000 [02:31<24:02,  1.59s/it]\u001b[A\n",
      "  9%|▉         | 92/1000 [02:32<21:46,  1.44s/it]\u001b[A\n",
      "  9%|▉         | 93/1000 [02:33<20:11,  1.34s/it]\u001b[A\n",
      " 10%|▉         | 95/1000 [02:33<14:31,  1.04it/s]\u001b[A\n",
      " 10%|▉         | 96/1000 [02:34<12:00,  1.25it/s]\u001b[A\n",
      " 10%|▉         | 97/1000 [02:39<30:17,  2.01s/it]\u001b[A\n",
      " 10%|▉         | 98/1000 [02:39<21:42,  1.44s/it]\u001b[A\n",
      " 10%|▉         | 99/1000 [02:39<15:53,  1.06s/it]\u001b[A\n",
      " 10%|█         | 100/1000 [02:39<11:56,  1.26it/s]\u001b[A\n",
      " 10%|█         | 102/1000 [02:39<08:51,  1.69it/s]\u001b[A\n",
      " 10%|█         | 103/1000 [02:40<08:23,  1.78it/s]\u001b[A\n",
      " 10%|█         | 104/1000 [02:40<08:38,  1.73it/s]\u001b[A\n",
      " 10%|█         | 105/1000 [02:41<10:18,  1.45it/s]\u001b[A\n",
      " 11%|█         | 106/1000 [02:42<08:28,  1.76it/s]\u001b[A\n",
      " 11%|█         | 107/1000 [02:43<14:09,  1.05it/s]\u001b[A\n",
      " 11%|█         | 108/1000 [02:44<12:27,  1.19it/s]\u001b[A\n",
      " 11%|█         | 109/1000 [02:46<18:26,  1.24s/it]\u001b[A\n",
      " 11%|█         | 110/1000 [02:51<32:54,  2.22s/it]\u001b[A\n",
      " 11%|█         | 111/1000 [02:52<27:03,  1.83s/it]\u001b[A\n",
      " 11%|█         | 112/1000 [02:56<36:28,  2.46s/it]\u001b[A\n",
      " 11%|█▏        | 113/1000 [02:58<35:12,  2.38s/it]\u001b[A\n",
      " 11%|█▏        | 114/1000 [02:58<27:10,  1.84s/it]\u001b[A\n",
      " 12%|█▏        | 115/1000 [02:59<23:13,  1.57s/it]\u001b[A\n",
      " 12%|█▏        | 116/1000 [03:00<18:35,  1.26s/it]\u001b[A\n",
      " 12%|█▏        | 117/1000 [03:01<18:30,  1.26s/it]\u001b[A\n",
      " 12%|█▏        | 118/1000 [03:03<22:33,  1.53s/it]\u001b[A\n",
      " 12%|█▏        | 119/1000 [03:04<17:45,  1.21s/it]\u001b[A\n",
      " 12%|█▏        | 120/1000 [03:05<17:56,  1.22s/it]\u001b[A\n",
      " 12%|█▏        | 122/1000 [03:07<16:02,  1.10s/it]\u001b[A\n",
      " 12%|█▏        | 123/1000 [03:08<17:47,  1.22s/it]\u001b[A\n",
      " 12%|█▏        | 124/1000 [03:09<14:24,  1.01it/s]\u001b[A\n",
      " 13%|█▎        | 126/1000 [03:10<14:09,  1.03it/s]\u001b[A\n",
      " 13%|█▎        | 127/1000 [03:11<10:36,  1.37it/s]\u001b[A\n",
      " 13%|█▎        | 128/1000 [03:12<13:53,  1.05it/s]\u001b[A\n",
      " 13%|█▎        | 130/1000 [03:12<10:02,  1.44it/s]\u001b[A\n",
      " 13%|█▎        | 131/1000 [03:13<08:48,  1.64it/s]\u001b[A\n",
      " 13%|█▎        | 132/1000 [03:15<16:05,  1.11s/it]\u001b[A\n",
      " 13%|█▎        | 133/1000 [03:15<12:11,  1.19it/s]\u001b[A\n",
      " 14%|█▎        | 135/1000 [03:16<10:28,  1.38it/s]\u001b[A\n",
      " 14%|█▎        | 136/1000 [03:20<24:13,  1.68s/it]\u001b[A\n",
      " 14%|█▎        | 137/1000 [03:20<17:32,  1.22s/it]\u001b[A\n",
      " 14%|█▍        | 138/1000 [03:23<24:36,  1.71s/it]\u001b[A\n",
      " 14%|█▍        | 139/1000 [03:23<18:35,  1.30s/it]\u001b[A\n",
      " 14%|█▍        | 140/1000 [03:28<32:03,  2.24s/it]\u001b[A\n",
      " 14%|█▍        | 141/1000 [03:31<36:16,  2.53s/it]\u001b[A\n",
      " 14%|█▍        | 142/1000 [03:34<36:57,  2.58s/it]\u001b[A\n",
      " 14%|█▍        | 143/1000 [03:34<27:44,  1.94s/it]\u001b[A\n",
      " 14%|█▍        | 144/1000 [03:35<22:47,  1.60s/it]\u001b[A\n",
      " 14%|█▍        | 145/1000 [03:36<19:09,  1.34s/it]\u001b[A\n",
      " 15%|█▍        | 146/1000 [03:38<25:16,  1.78s/it]\u001b[A\n",
      " 15%|█▍        | 147/1000 [03:39<18:09,  1.28s/it]\u001b[A\n",
      " 15%|█▍        | 149/1000 [03:39<13:08,  1.08it/s]\u001b[A\n",
      " 15%|█▌        | 150/1000 [03:41<19:54,  1.41s/it]\u001b[A\n",
      " 15%|█▌        | 151/1000 [03:42<16:23,  1.16s/it]\u001b[A\n",
      " 15%|█▌        | 152/1000 [03:43<14:53,  1.05s/it]\u001b[A\n",
      " 15%|█▌        | 153/1000 [03:43<12:40,  1.11it/s]\u001b[A\n",
      " 16%|█▌        | 155/1000 [03:43<09:13,  1.53it/s]\u001b[A\n",
      " 16%|█▌        | 156/1000 [03:47<20:15,  1.44s/it]\u001b[A\n",
      " 16%|█▌        | 157/1000 [03:47<16:29,  1.17s/it]\u001b[A\n",
      " 16%|█▌        | 158/1000 [03:49<17:49,  1.27s/it]\u001b[A\n",
      " 16%|█▌        | 159/1000 [03:50<17:40,  1.26s/it]\u001b[A\n",
      " 16%|█▌        | 160/1000 [03:50<13:18,  1.05it/s]\u001b[A\n",
      " 16%|█▌        | 161/1000 [03:54<23:30,  1.68s/it]\u001b[A\n",
      " 16%|█▌        | 162/1000 [03:54<17:28,  1.25s/it]\u001b[A\n",
      " 16%|█▋        | 163/1000 [03:56<20:48,  1.49s/it]\u001b[A\n",
      " 16%|█▋        | 164/1000 [03:57<18:30,  1.33s/it]\u001b[A\n",
      " 16%|█▋        | 165/1000 [03:57<14:06,  1.01s/it]\u001b[A\n",
      " 17%|█▋        | 166/1000 [04:02<29:54,  2.15s/it]\u001b[A\n",
      " 17%|█▋        | 167/1000 [04:03<23:43,  1.71s/it]\u001b[A\n",
      " 17%|█▋        | 168/1000 [04:03<18:16,  1.32s/it]\u001b[A\n",
      " 17%|█▋        | 169/1000 [04:04<18:17,  1.32s/it]\u001b[A\n",
      " 17%|█▋        | 170/1000 [04:06<20:47,  1.50s/it]\u001b[A\n",
      " 17%|█▋        | 172/1000 [04:06<14:55,  1.08s/it]\u001b[A\n",
      " 17%|█▋        | 173/1000 [04:08<15:02,  1.09s/it]\u001b[A\n",
      " 18%|█▊        | 175/1000 [04:10<14:53,  1.08s/it]\u001b[A\n",
      " 18%|█▊        | 176/1000 [04:12<19:52,  1.45s/it]\u001b[A\n",
      " 18%|█▊        | 177/1000 [04:16<28:35,  2.08s/it]\u001b[A\n",
      " 18%|█▊        | 178/1000 [04:17<24:50,  1.81s/it]\u001b[A\n",
      " 18%|█▊        | 179/1000 [04:21<34:49,  2.54s/it]\u001b[A\n",
      " 18%|█▊        | 180/1000 [04:21<25:30,  1.87s/it]\u001b[A\n",
      " 18%|█▊        | 181/1000 [04:21<18:18,  1.34s/it]\u001b[A\n",
      " 18%|█▊        | 182/1000 [04:22<14:14,  1.04s/it]\u001b[A\n",
      " 18%|█▊        | 184/1000 [04:22<10:23,  1.31it/s]\u001b[A\n",
      " 18%|█▊        | 185/1000 [04:23<09:47,  1.39it/s]\u001b[A\n",
      " 19%|█▊        | 186/1000 [04:29<32:40,  2.41s/it]\u001b[A\n",
      " 19%|█▊        | 187/1000 [04:29<24:04,  1.78s/it]\u001b[A\n",
      " 19%|█▉        | 188/1000 [04:30<20:18,  1.50s/it]\u001b[A\n",
      " 19%|█▉        | 189/1000 [04:31<19:53,  1.47s/it]\u001b[A\n",
      " 19%|█▉        | 191/1000 [04:33<16:32,  1.23s/it]\u001b[A\n",
      " 19%|█▉        | 192/1000 [04:36<23:55,  1.78s/it]\u001b[A\n",
      " 19%|█▉        | 194/1000 [04:37<18:17,  1.36s/it]\u001b[A\n",
      " 20%|█▉        | 195/1000 [04:38<16:39,  1.24s/it]\u001b[A\n",
      " 20%|█▉        | 196/1000 [04:39<16:18,  1.22s/it]\u001b[A\n",
      " 20%|█▉        | 197/1000 [04:39<12:54,  1.04it/s]\u001b[A\n",
      " 20%|█▉        | 198/1000 [04:42<20:31,  1.54s/it]\u001b[A\n",
      " 20%|█▉        | 199/1000 [04:42<14:59,  1.12s/it]\u001b[A\n",
      " 20%|██        | 200/1000 [04:43<14:35,  1.09s/it]\u001b[A\n",
      " 20%|██        | 201/1000 [04:49<31:48,  2.39s/it]\u001b[A\n",
      " 20%|██        | 202/1000 [04:49<25:22,  1.91s/it]\u001b[A\n",
      " 20%|██        | 203/1000 [04:50<20:00,  1.51s/it]\u001b[A\n",
      " 20%|██        | 204/1000 [04:50<16:04,  1.21s/it]\u001b[A\n",
      " 20%|██        | 205/1000 [04:56<31:21,  2.37s/it]\u001b[A\n",
      " 21%|██        | 206/1000 [04:56<25:05,  1.90s/it]\u001b[A\n",
      " 21%|██        | 207/1000 [05:00<31:33,  2.39s/it]\u001b[A\n",
      " 21%|██        | 208/1000 [05:01<24:57,  1.89s/it]\u001b[A\n",
      " 21%|██        | 209/1000 [05:01<19:13,  1.46s/it]\u001b[A\n",
      " 21%|██        | 210/1000 [05:02<18:03,  1.37s/it]\u001b[A\n",
      " 21%|██        | 211/1000 [05:02<13:04,  1.01it/s]\u001b[A\n",
      " 21%|██        | 212/1000 [05:03<13:10,  1.00s/it]\u001b[A\n",
      " 21%|██▏       | 214/1000 [05:05<12:10,  1.08it/s]\u001b[A\n",
      " 22%|██▏       | 215/1000 [05:06<11:10,  1.17it/s]\u001b[A\n",
      " 22%|██▏       | 216/1000 [05:07<12:07,  1.08it/s]\u001b[A\n",
      " 22%|██▏       | 217/1000 [05:07<09:12,  1.42it/s]\u001b[A\n",
      " 22%|██▏       | 218/1000 [05:10<20:21,  1.56s/it]\u001b[A\n",
      " 22%|██▏       | 219/1000 [05:10<14:41,  1.13s/it]\u001b[A\n",
      " 22%|██▏       | 220/1000 [05:11<11:50,  1.10it/s]\u001b[A\n",
      " 22%|██▏       | 221/1000 [05:11<08:54,  1.46it/s]\u001b[A\n",
      " 22%|██▏       | 222/1000 [05:11<07:29,  1.73it/s]\u001b[A\n",
      " 22%|██▏       | 223/1000 [05:12<05:57,  2.17it/s]\u001b[A\n",
      " 22%|██▏       | 224/1000 [05:13<09:58,  1.30it/s]\u001b[A\n",
      " 22%|██▎       | 225/1000 [05:13<07:55,  1.63it/s]\u001b[A\n",
      " 23%|██▎       | 226/1000 [05:17<17:58,  1.39s/it]\u001b[A\n",
      " 23%|██▎       | 227/1000 [05:18<18:40,  1.45s/it]\u001b[A\n",
      " 23%|██▎       | 228/1000 [05:27<45:45,  3.56s/it]\u001b[A\n",
      " 23%|██▎       | 229/1000 [05:27<34:02,  2.65s/it]\u001b[A\n",
      " 23%|██▎       | 230/1000 [05:27<25:13,  1.97s/it]\u001b[A\n",
      " 23%|██▎       | 231/1000 [05:30<28:44,  2.24s/it]\u001b[A\n",
      " 23%|██▎       | 232/1000 [05:31<21:30,  1.68s/it]\u001b[A\n",
      " 23%|██▎       | 233/1000 [05:40<50:51,  3.98s/it]\u001b[A\n",
      " 23%|██▎       | 234/1000 [05:47<1:02:55,  4.93s/it]\u001b[A\n",
      " 24%|██▎       | 235/1000 [05:49<49:41,  3.90s/it]  \u001b[A\n",
      " 24%|██▎       | 236/1000 [05:53<52:25,  4.12s/it]\u001b[A\n",
      " 24%|██▎       | 237/1000 [05:54<38:12,  3.00s/it]\u001b[A\n",
      " 24%|██▍       | 238/1000 [05:54<29:16,  2.31s/it]\u001b[A\n",
      " 24%|██▍       | 239/1000 [05:55<21:04,  1.66s/it]\u001b[A\n",
      " 24%|██▍       | 241/1000 [05:56<17:50,  1.41s/it]\u001b[A\n",
      " 24%|██▍       | 242/1000 [05:57<14:02,  1.11s/it]\u001b[A\n",
      " 24%|██▍       | 243/1000 [05:58<14:14,  1.13s/it]\u001b[A\n",
      " 24%|██▍       | 244/1000 [06:03<28:06,  2.23s/it]\u001b[A\n",
      " 24%|██▍       | 245/1000 [06:05<28:36,  2.27s/it]\u001b[A\n",
      " 25%|██▍       | 246/1000 [06:10<38:07,  3.03s/it]\u001b[A\n",
      " 25%|██▍       | 247/1000 [06:10<27:15,  2.17s/it]\u001b[A\n",
      " 25%|██▍       | 249/1000 [06:12<23:02,  1.84s/it]\u001b[A\n",
      " 25%|██▌       | 250/1000 [06:13<19:25,  1.55s/it]\u001b[A\n",
      " 25%|██▌       | 251/1000 [06:13<14:10,  1.14s/it]\u001b[A\n",
      " 25%|██▌       | 252/1000 [06:14<12:39,  1.02s/it]\u001b[A\n",
      " 25%|██▌       | 253/1000 [06:15<12:37,  1.01s/it]\u001b[A\n",
      " 25%|██▌       | 254/1000 [06:16<14:41,  1.18s/it]\u001b[A\n",
      " 26%|██▌       | 255/1000 [06:17<13:12,  1.06s/it]\u001b[A\n",
      " 26%|██▌       | 256/1000 [06:18<11:46,  1.05it/s]\u001b[A\n",
      " 26%|██▌       | 257/1000 [06:18<09:16,  1.33it/s]\u001b[A\n",
      " 26%|██▌       | 258/1000 [06:21<15:29,  1.25s/it]\u001b[A\n",
      " 26%|██▌       | 259/1000 [06:21<11:19,  1.09it/s]\u001b[A\n",
      " 26%|██▌       | 260/1000 [06:25<23:35,  1.91s/it]\u001b[A\n",
      " 26%|██▌       | 261/1000 [06:29<32:46,  2.66s/it]\u001b[A\n",
      " 26%|██▌       | 262/1000 [06:38<52:47,  4.29s/it]\u001b[A\n",
      " 26%|██▋       | 263/1000 [06:44<59:59,  4.88s/it]\u001b[A\n",
      " 26%|██▋       | 264/1000 [06:44<43:35,  3.55s/it]\u001b[A\n",
      " 26%|██▋       | 265/1000 [06:45<32:47,  2.68s/it]\u001b[A\n",
      " 27%|██▋       | 266/1000 [06:46<27:28,  2.25s/it]\u001b[A\n",
      " 27%|██▋       | 267/1000 [06:46<19:47,  1.62s/it]\u001b[A\n",
      " 27%|██▋       | 268/1000 [06:48<21:21,  1.75s/it]\u001b[A\n",
      " 27%|██▋       | 269/1000 [06:51<24:49,  2.04s/it]\u001b[A\n",
      " 27%|██▋       | 270/1000 [07:04<1:03:35,  5.23s/it]\u001b[A\n",
      " 27%|██▋       | 272/1000 [07:04<44:46,  3.69s/it]  \u001b[A\n",
      " 27%|██▋       | 273/1000 [07:04<32:11,  2.66s/it]\u001b[A\n",
      " 27%|██▋       | 274/1000 [07:05<26:00,  2.15s/it]\u001b[A\n",
      " 28%|██▊       | 275/1000 [07:07<25:58,  2.15s/it]\u001b[A\n",
      " 28%|██▊       | 276/1000 [07:07<18:43,  1.55s/it]\u001b[A\n",
      " 28%|██▊       | 277/1000 [07:08<15:01,  1.25s/it]\u001b[A\n",
      " 28%|██▊       | 278/1000 [07:09<14:43,  1.22s/it]\u001b[A\n",
      " 28%|██▊       | 279/1000 [07:10<14:13,  1.18s/it]\u001b[A\n",
      " 28%|██▊       | 280/1000 [07:11<11:51,  1.01it/s]\u001b[A\n",
      " 28%|██▊       | 281/1000 [07:12<12:45,  1.06s/it]\u001b[A\n",
      " 28%|██▊       | 282/1000 [07:14<17:37,  1.47s/it]\u001b[A\n",
      " 28%|██▊       | 283/1000 [07:15<13:05,  1.10s/it]\u001b[A\n",
      " 28%|██▊       | 284/1000 [07:16<12:21,  1.04s/it]\u001b[A\n",
      " 28%|██▊       | 285/1000 [07:18<17:13,  1.45s/it]\u001b[A\n",
      " 29%|██▊       | 286/1000 [07:19<16:42,  1.40s/it]\u001b[A\n",
      " 29%|██▊       | 287/1000 [07:27<40:02,  3.37s/it]\u001b[A\n",
      " 29%|██▉       | 288/1000 [07:28<32:08,  2.71s/it]\u001b[A\n",
      " 29%|██▉       | 289/1000 [07:32<35:32,  3.00s/it]\u001b[A\n",
      " 29%|██▉       | 290/1000 [07:32<26:19,  2.22s/it]\u001b[A\n",
      " 29%|██▉       | 291/1000 [07:35<27:24,  2.32s/it]\u001b[A\n",
      " 29%|██▉       | 292/1000 [07:38<28:39,  2.43s/it]\u001b[A\n",
      " 29%|██▉       | 293/1000 [07:39<23:50,  2.02s/it]\u001b[A\n",
      " 29%|██▉       | 294/1000 [07:40<22:11,  1.89s/it]\u001b[A\n",
      " 30%|██▉       | 295/1000 [07:41<18:50,  1.60s/it]\u001b[A\n",
      " 30%|██▉       | 296/1000 [07:45<24:32,  2.09s/it]\u001b[A\n",
      " 30%|██▉       | 297/1000 [07:45<19:21,  1.65s/it]\u001b[A\n",
      " 30%|██▉       | 298/1000 [07:46<15:31,  1.33s/it]\u001b[A\n",
      " 30%|██▉       | 299/1000 [07:46<11:19,  1.03it/s]\u001b[A\n",
      " 30%|███       | 300/1000 [07:47<10:16,  1.13it/s]\u001b[A\n",
      " 30%|███       | 301/1000 [07:48<12:19,  1.06s/it]\u001b[A\n",
      " 30%|███       | 302/1000 [07:49<12:24,  1.07s/it]\u001b[A\n",
      " 30%|███       | 303/1000 [07:51<14:28,  1.25s/it]\u001b[A\n",
      " 30%|███       | 304/1000 [07:52<13:59,  1.21s/it]\u001b[A\n",
      " 30%|███       | 305/1000 [07:53<14:26,  1.25s/it]\u001b[A\n",
      " 31%|███       | 306/1000 [07:58<26:16,  2.27s/it]\u001b[A\n",
      " 31%|███       | 307/1000 [07:59<23:48,  2.06s/it]\u001b[A\n",
      " 31%|███       | 308/1000 [08:00<17:12,  1.49s/it]\u001b[A\n",
      " 31%|███       | 309/1000 [08:01<15:34,  1.35s/it]\u001b[A\n",
      " 31%|███       | 310/1000 [08:03<20:46,  1.81s/it]\u001b[A\n",
      " 31%|███       | 311/1000 [08:04<16:40,  1.45s/it]\u001b[A\n",
      " 31%|███       | 312/1000 [08:06<18:38,  1.63s/it]\u001b[A\n",
      " 31%|███▏      | 313/1000 [08:06<14:17,  1.25s/it]\u001b[A\n",
      " 31%|███▏      | 314/1000 [08:07<13:15,  1.16s/it]\u001b[A\n",
      " 32%|███▏      | 315/1000 [08:09<13:01,  1.14s/it]\u001b[A\n",
      " 32%|███▏      | 316/1000 [08:09<12:08,  1.06s/it]\u001b[A\n",
      " 32%|███▏      | 317/1000 [08:11<15:27,  1.36s/it]\u001b[A\n",
      " 32%|███▏      | 318/1000 [08:13<16:53,  1.49s/it]\u001b[A\n",
      " 32%|███▏      | 319/1000 [08:13<12:26,  1.10s/it]\u001b[A\n",
      " 32%|███▏      | 320/1000 [08:22<39:09,  3.46s/it]\u001b[A\n",
      " 32%|███▏      | 321/1000 [08:25<36:53,  3.26s/it]\u001b[A\n",
      " 32%|███▏      | 322/1000 [08:25<26:15,  2.32s/it]\u001b[A\n",
      " 32%|███▏      | 323/1000 [08:26<21:48,  1.93s/it]\u001b[A\n",
      " 32%|███▏      | 324/1000 [08:27<18:03,  1.60s/it]\u001b[A\n",
      " 32%|███▎      | 325/1000 [08:33<31:40,  2.82s/it]\u001b[A\n",
      " 33%|███▎      | 326/1000 [08:37<36:19,  3.23s/it]\u001b[A\n",
      " 33%|███▎      | 327/1000 [08:39<30:41,  2.74s/it]\u001b[A\n",
      " 33%|███▎      | 328/1000 [08:39<21:54,  1.96s/it]\u001b[A\n",
      " 33%|███▎      | 329/1000 [08:40<17:46,  1.59s/it]\u001b[A\n",
      " 33%|███▎      | 330/1000 [08:41<17:24,  1.56s/it]\u001b[A\n",
      " 33%|███▎      | 331/1000 [08:47<31:39,  2.84s/it]\u001b[A\n",
      " 33%|███▎      | 332/1000 [08:48<26:05,  2.34s/it]\u001b[A\n",
      " 33%|███▎      | 333/1000 [08:49<21:56,  1.97s/it]\u001b[A\n",
      " 33%|███▎      | 334/1000 [08:50<16:49,  1.52s/it]\u001b[A\n",
      " 34%|███▎      | 335/1000 [08:50<13:50,  1.25s/it]\u001b[A\n",
      " 34%|███▎      | 337/1000 [08:52<13:25,  1.21s/it]\u001b[A\n",
      " 34%|███▍      | 338/1000 [08:55<18:26,  1.67s/it]\u001b[A\n",
      " 34%|███▍      | 339/1000 [08:58<22:02,  2.00s/it]\u001b[A\n",
      " 34%|███▍      | 340/1000 [08:58<16:39,  1.51s/it]\u001b[A\n",
      " 34%|███▍      | 341/1000 [08:59<12:27,  1.13s/it]\u001b[A\n",
      " 34%|███▍      | 342/1000 [09:01<15:06,  1.38s/it]\u001b[A\n",
      " 34%|███▍      | 343/1000 [09:08<33:47,  3.09s/it]\u001b[A\n",
      " 34%|███▍      | 344/1000 [09:08<26:03,  2.38s/it]\u001b[A\n",
      " 34%|███▍      | 345/1000 [09:14<35:46,  3.28s/it]\u001b[A\n",
      " 35%|███▍      | 346/1000 [09:14<26:03,  2.39s/it]\u001b[A\n",
      " 35%|███▍      | 347/1000 [09:15<22:32,  2.07s/it]\u001b[A\n",
      " 35%|███▍      | 348/1000 [09:17<20:55,  1.93s/it]\u001b[A\n",
      " 35%|███▍      | 349/1000 [09:20<23:50,  2.20s/it]\u001b[A\n",
      " 35%|███▌      | 350/1000 [09:25<34:09,  3.15s/it]\u001b[A\n",
      " 35%|███▌      | 351/1000 [09:26<25:45,  2.38s/it]\u001b[A\n",
      " 35%|███▌      | 352/1000 [09:26<18:26,  1.71s/it]\u001b[A\n",
      " 35%|███▌      | 353/1000 [09:27<17:41,  1.64s/it]\u001b[A\n",
      " 35%|███▌      | 354/1000 [09:28<13:04,  1.21s/it]\u001b[A\n",
      " 36%|███▌      | 355/1000 [09:31<20:31,  1.91s/it]\u001b[A\n",
      " 36%|███▌      | 356/1000 [09:33<19:48,  1.85s/it]\u001b[A\n",
      " 36%|███▌      | 357/1000 [09:35<20:05,  1.88s/it]\u001b[A\n",
      " 36%|███▌      | 358/1000 [09:37<20:03,  1.87s/it]\u001b[A\n",
      " 36%|███▌      | 359/1000 [09:37<16:43,  1.56s/it]\u001b[A\n",
      " 36%|███▌      | 360/1000 [09:38<12:29,  1.17s/it]\u001b[A\n",
      " 36%|███▌      | 361/1000 [09:42<22:50,  2.15s/it]\u001b[A\n",
      " 36%|███▌      | 362/1000 [09:43<18:48,  1.77s/it]\u001b[A\n",
      " 36%|███▋      | 364/1000 [09:44<14:06,  1.33s/it]\u001b[A\n",
      " 37%|███▋      | 366/1000 [09:44<10:07,  1.04it/s]\u001b[A\n",
      " 37%|███▋      | 367/1000 [09:45<10:30,  1.00it/s]\u001b[A\n",
      " 37%|███▋      | 368/1000 [09:46<10:07,  1.04it/s]\u001b[A\n",
      " 37%|███▋      | 369/1000 [09:49<18:08,  1.73s/it]\u001b[A\n",
      " 37%|███▋      | 370/1000 [09:50<14:21,  1.37s/it]\u001b[A\n",
      " 37%|███▋      | 371/1000 [09:56<30:25,  2.90s/it]\u001b[A\n",
      " 37%|███▋      | 372/1000 [09:57<22:55,  2.19s/it]\u001b[A\n",
      " 37%|███▋      | 373/1000 [09:57<17:09,  1.64s/it]\u001b[A\n",
      " 37%|███▋      | 374/1000 [10:00<20:54,  2.00s/it]\u001b[A\n",
      " 38%|███▊      | 375/1000 [10:02<19:29,  1.87s/it]\u001b[A\n",
      " 38%|███▊      | 376/1000 [10:02<14:53,  1.43s/it]\u001b[A\n",
      " 38%|███▊      | 377/1000 [10:02<11:46,  1.13s/it]\u001b[A\n",
      " 38%|███▊      | 379/1000 [10:04<10:52,  1.05s/it]\u001b[A\n",
      " 38%|███▊      | 380/1000 [10:05<08:58,  1.15it/s]\u001b[A\n",
      " 38%|███▊      | 381/1000 [10:05<06:55,  1.49it/s]\u001b[A\n",
      " 38%|███▊      | 382/1000 [10:06<09:57,  1.03it/s]\u001b[A\n",
      " 38%|███▊      | 383/1000 [10:09<14:05,  1.37s/it]\u001b[A\n",
      " 38%|███▊      | 384/1000 [10:13<24:02,  2.34s/it]\u001b[A\n",
      " 38%|███▊      | 385/1000 [10:15<21:53,  2.14s/it]\u001b[A\n",
      " 39%|███▊      | 386/1000 [10:15<15:57,  1.56s/it]\u001b[A\n",
      " 39%|███▊      | 387/1000 [10:15<11:30,  1.13s/it]\u001b[A\n",
      " 39%|███▉      | 388/1000 [10:16<10:36,  1.04s/it]\u001b[A\n",
      " 39%|███▉      | 390/1000 [10:21<14:49,  1.46s/it]\u001b[A\n",
      " 39%|███▉      | 391/1000 [10:22<13:53,  1.37s/it]\u001b[A\n",
      " 39%|███▉      | 392/1000 [10:24<13:27,  1.33s/it]\u001b[A\n",
      " 39%|███▉      | 393/1000 [10:24<10:38,  1.05s/it]\u001b[A\n",
      " 39%|███▉      | 394/1000 [10:24<09:11,  1.10it/s]\u001b[A\n",
      " 40%|███▉      | 395/1000 [10:25<09:28,  1.06it/s]\u001b[A\n",
      " 40%|███▉      | 396/1000 [10:26<08:37,  1.17it/s]\u001b[A\n",
      " 40%|███▉      | 398/1000 [10:28<08:15,  1.22it/s]\u001b[A\n",
      " 40%|███▉      | 399/1000 [10:40<41:47,  4.17s/it]\u001b[A\n",
      " 40%|████      | 400/1000 [10:40<29:46,  2.98s/it]\u001b[A\n",
      " 40%|████      | 401/1000 [10:41<23:12,  2.32s/it]\u001b[A\n",
      " 40%|████      | 402/1000 [10:43<23:30,  2.36s/it]\u001b[A\n",
      " 40%|████      | 403/1000 [10:43<16:54,  1.70s/it]\u001b[A\n",
      " 40%|████      | 404/1000 [10:43<12:32,  1.26s/it]\u001b[A\n",
      " 40%|████      | 405/1000 [10:46<17:36,  1.78s/it]\u001b[A\n",
      " 41%|████      | 406/1000 [10:48<17:31,  1.77s/it]\u001b[A\n",
      " 41%|████      | 407/1000 [10:49<13:49,  1.40s/it]\u001b[A\n",
      " 41%|████      | 408/1000 [10:52<18:33,  1.88s/it]\u001b[A\n",
      " 41%|████      | 409/1000 [10:53<16:12,  1.65s/it]\u001b[A\n",
      " 41%|████      | 410/1000 [10:55<17:23,  1.77s/it]\u001b[A\n",
      " 41%|████      | 411/1000 [10:55<13:06,  1.34s/it]\u001b[A\n",
      " 41%|████▏     | 413/1000 [10:55<09:23,  1.04it/s]\u001b[A\n",
      " 41%|████▏     | 414/1000 [10:56<08:18,  1.18it/s]\u001b[A\n",
      " 42%|████▏     | 415/1000 [10:57<07:20,  1.33it/s]\u001b[A\n",
      " 42%|████▏     | 416/1000 [10:59<12:19,  1.27s/it]\u001b[A\n",
      " 42%|████▏     | 417/1000 [10:59<09:20,  1.04it/s]\u001b[A\n",
      " 42%|████▏     | 419/1000 [11:00<07:34,  1.28it/s]\u001b[A\n",
      " 42%|████▏     | 420/1000 [11:00<06:08,  1.57it/s]\u001b[A\n",
      " 42%|████▏     | 421/1000 [11:01<06:05,  1.58it/s]\u001b[A\n",
      " 42%|████▏     | 422/1000 [11:01<06:01,  1.60it/s]\u001b[A\n",
      " 42%|████▏     | 423/1000 [11:03<07:20,  1.31it/s]\u001b[A\n",
      " 42%|████▏     | 424/1000 [11:03<06:31,  1.47it/s]\u001b[A\n",
      " 42%|████▎     | 425/1000 [11:04<07:41,  1.24it/s]\u001b[A\n",
      " 43%|████▎     | 426/1000 [11:05<07:10,  1.33it/s]\u001b[A\n",
      " 43%|████▎     | 427/1000 [11:11<21:46,  2.28s/it]\u001b[A\n",
      " 43%|████▎     | 428/1000 [11:11<16:22,  1.72s/it]\u001b[A\n",
      " 43%|████▎     | 429/1000 [11:12<13:51,  1.46s/it]\u001b[A\n",
      " 43%|████▎     | 430/1000 [11:13<12:59,  1.37s/it]\u001b[A\n",
      " 43%|████▎     | 431/1000 [11:19<27:28,  2.90s/it]\u001b[A\n",
      " 43%|████▎     | 432/1000 [11:21<23:39,  2.50s/it]\u001b[A\n",
      " 43%|████▎     | 433/1000 [11:21<17:03,  1.80s/it]\u001b[A\n",
      " 43%|████▎     | 434/1000 [11:21<12:31,  1.33s/it]\u001b[A\n",
      " 44%|████▎     | 435/1000 [11:22<09:12,  1.02it/s]\u001b[A\n",
      " 44%|████▎     | 436/1000 [11:24<12:12,  1.30s/it]\u001b[A\n",
      " 44%|████▎     | 437/1000 [11:28<19:24,  2.07s/it]\u001b[A\n",
      " 44%|████▍     | 438/1000 [11:28<15:03,  1.61s/it]\u001b[A\n",
      " 44%|████▍     | 439/1000 [11:30<14:39,  1.57s/it]\u001b[A\n",
      " 44%|████▍     | 440/1000 [11:34<22:30,  2.41s/it]\u001b[A\n",
      " 44%|████▍     | 441/1000 [11:37<24:36,  2.64s/it]\u001b[A\n",
      " 44%|████▍     | 442/1000 [11:38<18:56,  2.04s/it]\u001b[A\n",
      " 44%|████▍     | 443/1000 [11:39<15:31,  1.67s/it]\u001b[A\n",
      " 44%|████▍     | 444/1000 [11:39<12:51,  1.39s/it]\u001b[A\n",
      " 44%|████▍     | 445/1000 [11:41<12:52,  1.39s/it]\u001b[A\n",
      " 45%|████▍     | 446/1000 [11:41<09:26,  1.02s/it]\u001b[A\n",
      " 45%|████▍     | 448/1000 [11:41<06:52,  1.34it/s]\u001b[A\n",
      " 45%|████▍     | 449/1000 [11:42<06:31,  1.41it/s]\u001b[A\n",
      " 45%|████▌     | 450/1000 [11:43<08:26,  1.09it/s]\u001b[A\n",
      " 45%|████▌     | 451/1000 [11:45<12:07,  1.33s/it]\u001b[A\n",
      " 45%|████▌     | 452/1000 [11:46<09:48,  1.07s/it]\u001b[A\n",
      " 45%|████▌     | 453/1000 [11:46<07:21,  1.24it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f7ac14d890fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_examples_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mattention_relevance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_flow_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_examples_attentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mall_examples_flow_relevance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_relevance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d0afe955ab3f>\u001b[0m in \u001b[0;36mget_flow_relevance\u001b[0;34m(full_att_mat, input_tokens, layer)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0minput_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mflow_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_node_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_labels_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_att_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_att_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/attention_flow/attention_graph_util.py\u001b[0m in \u001b[0;36mcompute_node_flow\u001b[0;34m(G, labels_to_index, input_nodes, output_nodes, length)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minp_node_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp_node_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mflow_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum_flow_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medmonds_karp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0mflow_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpre_layer\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mflow_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mflow_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/networkx/algorithms/flow/maxflow.py\u001b[0m in \u001b[0;36mmaximum_flow_value\u001b[0;34m(flowG, _s, _t, capacity, flow_func, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetworkXError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"flow_func has to be callable.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflowG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcapacity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flow_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/networkx/algorithms/flow/edmondskarp.py\u001b[0m in \u001b[0;36medmonds_karp\u001b[0;34m(G, s, t, capacity, residual, value_only, cutoff)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medmonds_karp_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'algorithm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'edmonds_karp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/networkx/algorithms/flow/edmondskarp.py\u001b[0m in \u001b[0;36medmonds_karp_impl\u001b[0;34m(G, s, t, capacity, residual, cutoff)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_residual_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/networkx/algorithms/flow/utils.py\u001b[0m in \u001b[0;36mbuild_residual_network\u001b[0;34m(G, capacity)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;31m# Both (u, v) and (v, u) must be present in the residual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;31m# network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"compute raw relevance scores ...\")\n",
    "all_examples_raw_relevance = {}\n",
    "for l in np.arange(0,6):\n",
    "    all_examples_raw_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.decode(all_examples_x[i][0].numpy())\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_raw_att_relevance(all_examples_attentions[i][...,:length, :length], tokens, layer=l)\n",
    "        all_examples_raw_relevance[l].append(np.asarray(attention_relevance))\n",
    "\n",
    "print(\"compute joint relevance scores ...\")\n",
    "all_examples_joint_relevance = {}\n",
    "for l in np.arange(0,6):\n",
    "    all_examples_joint_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.decode(all_examples_x[i][0].numpy())\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_joint_relevance(all_examples_attentions[i][...,:length, :length], tokens, layer=l)\n",
    "        all_examples_joint_relevance[l].append(np.asarray(attention_relevance))\n",
    "    \n",
    "print(\"compute flow relevance scores ...\")\n",
    "all_examples_flow_relevance = {}\n",
    "for l in np.arange(0,6):\n",
    "    all_examples_flow_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.decode(all_examples_x[i][0].numpy())\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_flow_relevance(all_examples_attentions[i][...,:length, :length], tokens, layer=l)\n",
    "        all_examples_flow_relevance[l].append(np.asarray(attention_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############Layer  0 #############\n",
      "raw blankout\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_examples_raw_relevance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7df5286892ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_examples_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspearmanr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_examples_raw_relevance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_examples_blankout_relevance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0msps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_examples_raw_relevance' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(np.mean([spearmanr(all_examples_flow_relevance[i], all_examples_joint_relevance[i]) for i in np.arange(len(all_examples_x))]))\n",
    "# print(np.mean([spearmanr(all_examples_flow_relevance[i], all_examples_blankout_relevance[i]) for i in np.arange(len(all_examples_x))]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for l in np.arange(0,6):\n",
    "    print(\"###############Layer \",l, \"#############\")\n",
    "    print('raw blankout')\n",
    "    sps = []\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_raw_relevance[l][i],all_examples_blankout_relevance[i], axis=1)\n",
    "        if not math.isnan(sp[0]):\n",
    "            sps.append(sp)\n",
    "        else:\n",
    "            sps.append(0)\n",
    "        \n",
    "    print(np.mean(sps))\n",
    "    \n",
    "    \n",
    "    print('raw inputgrad')\n",
    "    sps = []\n",
    "    print(all_examples_raw_relevance[l][0].shape, all_examples_inputgrads[0][0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_raw_relevance[l][i],all_examples_inputgrads[i][0], axis=1)\n",
    "        if not math.isnan(sp[0]):\n",
    "            sps.append(sp)\n",
    "        else:\n",
    "            sps.append(0)\n",
    "        \n",
    "    print(np.mean(sps))\n",
    "    \n",
    "    print('raw grad')\n",
    "    sps = []\n",
    "    print(all_examples_raw_relevance[l][0].shape, all_examples_grads[0][0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_raw_relevance[l][i],all_examples_grads[i][0], axis=1)\n",
    "        if not math.isnan(sp[0]):\n",
    "            sps.append(sp)\n",
    "        else:\n",
    "            sps.append(0)\n",
    "        \n",
    "    print(np.mean(sps))\n",
    "    \n",
    "    print('joint blankout')\n",
    "    sps = []\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_joint_relevance[l][i],all_examples_blankout_relevance[i])\n",
    "        if not math.isnan(sp[0]):\n",
    "            sps.append(sp)\n",
    "        else:\n",
    "            sps.append(0)\n",
    "        \n",
    "    print(np.mean(sps))\n",
    "    \n",
    "    print('joint grad')\n",
    "    sps = []\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_examples_grads[0][0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_joint_relevance[l][i],all_examples_grads[i][0], axis=1)\n",
    "        if not math.isnan(sp[0]):\n",
    "            sps.append(sp)\n",
    "        else:\n",
    "            sps.append(0)\n",
    "        \n",
    "    print(np.mean(sps))\n",
    "    \n",
    "    print('joint inputgrad')\n",
    "    sps = []\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_examples_inputgrads[0][0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_joint_relevance[l][i],all_examples_inputgrads[i][0], axis=1)\n",
    "        if not math.isnan(sp[0]):\n",
    "            sps.append(sp)\n",
    "        else:\n",
    "            sps.append(0)\n",
    "        \n",
    "    print(np.mean(sps))\n",
    "    \n",
    "    print('flow')\n",
    "    sps = []\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_flow_relevance[l][i],all_examples_blankout_relevance[i])\n",
    "        \n",
    "        if not math.isnan(sp[0]):\n",
    "            sps.append(sp)\n",
    "        else:\n",
    "            sps.append(0)\n",
    "        \n",
    "    print(np.mean(sps))\n",
    "  \n",
    "    print('flow grad')\n",
    "    sps = []\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_examples_grads[0][0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_flow_relevance[l][i],all_examples_grads[i][0], axis=1)\n",
    "        if not math.isnan(sp[0]):\n",
    "            sps.append(sp)\n",
    "        else:\n",
    "            sps.append(0)\n",
    "        \n",
    "    print(np.mean(sps))\n",
    "    \n",
    "    print('flow inputgrad')\n",
    "    sps = []\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_examples_inputgrads[0][0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_flow_relevance[l][i],all_examples_inputgrads[i][0], axis=1)\n",
    "        if not math.isnan(sp[0]):\n",
    "            sps.append(sp)\n",
    "        else:\n",
    "            sps.append(0)\n",
    "        \n",
    "    print(np.mean(sps))\n",
    "        \n",
    "\n",
    "# for l in np.arange(6):\n",
    "#     print(\"layer \",l)\n",
    "#     print(all_examples_blankout_relevance[0].numpy().shape, all_examples_raw_relevance[l][0].shape, all_examples_joint_relevance[l][0].shape)\n",
    "#     print('raw:',np.mean([spearmanr(all_examples_blankout_relevance[i], all_examples_blankout_relevance[i].numpy()) for i in np.arange(len(all_examples_x))]))\n",
    "#     print('joint',np.mean([spearmanr(all_examples_joint_relevance[l][i], all_examples_blankout_relevance[i].numpy()) for i in np.arange(len(all_examples_x))]))\n",
    "#     #print('flow',np.mean([spearmanr(all_examples_flow_relevance[l][i], all_examples_blankout_relevance[l][i]) for i in np.arange(len(all_examples_x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
