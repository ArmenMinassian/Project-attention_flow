{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/samira/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from attention_graph_util import *\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "import matplotlib as mpl\n",
    "import networkx as nx\n",
    "import os\n",
    "from util import constants\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import pandas as pd\n",
    "\n",
    "from util.models import MODELS\n",
    "from util.tasks import TASKS\n",
    "#from dnotebook_utils import *\n",
    "from attention_graph_util import *\n",
    "%matplotlib inline\n",
    "from util.config_util import get_task_params\n",
    "from notebooks.notebook_utils import *\n",
    "from util import inflect\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "rc={'font.size': 10, 'axes.labelsize': 10, 'legend.fontsize': 10.0, \n",
    "    'axes.titlesize': 32, 'xtick.labelsize': 20, 'ytick.labelsize': 16}\n",
    "plt.rcParams.update(**rc)\n",
    "mpl.rcParams['axes.linewidth'] = .5 #set the value globally\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: transformers in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (4.36.1)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (2019.11.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/samira/.local/lib/python3.7/site-packages (from transformers) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (0.1.83)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers==0.5.2 in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from transformers) (1.10.28)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from requests->transformers) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/samira/.local/lib/python3.7/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.28 in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from boto3->transformers) (1.13.28)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.28->boto3->transformers) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/samira/anaconda3/envs/indist/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.28->boto3->transformers) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n",
      "INFO:absl:Load dataset info from ../InDist/data/word_sv_agreement/0.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab len:  10032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Constructing tf.data.Dataset for split validation, from ../InDist/data/word_sv_agreement/0.1.0\n",
      "INFO:absl:Constructing tf.data.Dataset for split test, from ../InDist/data/word_sv_agreement/0.1.0\n",
      "INFO:absl:Constructing tf.data.Dataset for split train, from ../InDist/data/word_sv_agreement/0.1.0\n"
     ]
    }
   ],
   "source": [
    "task_name = 'word_sv_agreement_lm'\n",
    "task_params = get_task_params(batch_size=1)\n",
    "task = TASKS[task_name](task_params, data_dir='../InDist/data')\n",
    "cl_token = task.sentence_encoder().encode(constants.bos)\n",
    "task_tokenizer = task.sentence_encoder()._tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers has a unified API\n",
    "# for 8 transformer architectures and 30 pretrained weights.\n",
    "#          Model          | Tokenizer          | Pretrained weights shortcut\n",
    "MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),\n",
    "          (OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'),\n",
    "          (GPT2Model,       GPT2Tokenizer,       'gpt2'),\n",
    "          (CTRLModel,       CTRLTokenizer,       'ctrl'),\n",
    "          (TransfoXLModel,  TransfoXLTokenizer,  'transfo-xl-wt103'),\n",
    "          (XLNetModel,      XLNetTokenizer,      'xlnet-base-cased'),\n",
    "          (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n",
    "          (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased'),\n",
    "          (RobertaModel,    RobertaTokenizer,    'roberta-base')]\n",
    "\n",
    "# Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.\n",
    "BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n",
    "                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]\n",
    "\n",
    "# All the classes for an architecture can be initiated from pretrained weights for this architecture\n",
    "# Note that additional weights added for fine-tuning are only initialized\n",
    "# and need to be trained on the down-stream task\n",
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(pretrained_weights,\n",
    "                                  output_hidden_states=True,\n",
    "                                  output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_convertor(encoded_input_task, task_offset, task_encoder, tokenizer):\n",
    "    string_part1 = task_encoder.decode(encoded_input_task[:task_offset])\n",
    "    tokens_part1 = tokenizer.tokenize(string_part1)\n",
    "    \n",
    "    return len(tokens_part1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many NNS of woodland remain and support a JJ sector in the southern portion of the state .\n",
      "21 ['cls', 'many', 'n', '##ns', 'of', 'woodland', 'remain', 'and', 'support', 'a', 'jj', 'sector', 'in', 'the', 'southern', 'portion', 'of', 'the', 'state', '.', 'sep']\n",
      "(12, 12, 21, 21)\n"
     ]
    }
   ],
   "source": [
    "for x,y in task.test_dataset:\n",
    "    sentence = task.sentence_encoder().decode(x[0][1:])\n",
    "    print(sentence)\n",
    "    break\n",
    "\n",
    "tokens = ['cls']+tokenizer.tokenize(sentence)+['sep']\n",
    "print(len(tokens), tokens)\n",
    "tf_input_ids = tokenizer.encode(sentence)\n",
    "input_ids = torch.tensor([tf_input_ids])\n",
    "all_hidden_states, all_attentions = model(input_ids)[-2:]\n",
    "\n",
    "_attentions = [att.detach().numpy() for att in all_attentions]\n",
    "attentions_mat = np.asarray(_attentions)[:,0]\n",
    "print(attentions_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Constructing tf.data.Dataset for split validation, from ../InDist/data/word_sv_agreement/0.1.0\n",
      "552it [09:51,  1.47s/it]"
     ]
    }
   ],
   "source": [
    "all_examples_x = []\n",
    "all_examples_y = []\n",
    "all_examples_attentions = []\n",
    "all_examples_blankout_relevance = []\n",
    "\n",
    "n_batches = 1000\n",
    "\n",
    "\n",
    "infl_eng = inflect.engine()\n",
    "verb_infl, noun_infl = gen_inflect_from_vocab(infl_eng, '../InDist/notebooks/wiki.vocab')\n",
    "\n",
    "test_data = task.databuilder.as_dataset(split='validation', batch_size=1)\n",
    "for examples in tqdm(test_data):\n",
    "    sentence = task.sentence_encoder().decode(examples['sentence'][0])\n",
    "    \n",
    "    verb_position = examples['verb_position'][0].numpy()+1  #+1 because of adding cls.\n",
    "    verb_position = offset_convertor(examples['sentence'][0], verb_position, task.sentence_encoder(), tokenizer)\n",
    "    \n",
    "    sentence = ['cls']+tokenizer.tokenize(sentence)+['sep']\n",
    "    \n",
    "    \n",
    "    sentence[verb_position] = tokenizer.mask_token\n",
    "    tf_input_ids = tokenizer.encode(sentence)\n",
    "    input_ids = torch.tensor([tf_input_ids])\n",
    "    \n",
    "\n",
    "    \n",
    "    s_shape = input_ids.shape\n",
    "    batch_size, length = s_shape[0], s_shape[1]\n",
    "    actual_verb = examples['verb'][0].numpy().decode(\"utf-8\")\n",
    "    inflected_verb = verb_infl[actual_verb] \n",
    "\n",
    "\n",
    "    actual_verb_index = tokenizer.encode(tokenizer.tokenize(actual_verb))[1]\n",
    "    inflected_verb_index = tokenizer.encode(tokenizer.tokenize(inflected_verb))[1]\n",
    "\n",
    "    all_examples_x.append(input_ids)\n",
    "    predictions = model(input_ids)\n",
    "    logits = predictions[0][0]\n",
    "    probs = torch.nn.Softmax(dim=-1)(logits)\n",
    "    actual_verb_score = probs[verb_position][actual_verb_index]\n",
    "    inflected_verb_score = probs[verb_position][inflected_verb_index]\n",
    "    \n",
    "    main_diff_score = actual_verb_score - inflected_verb_score\n",
    "    hidden_states, attentions = predictions[-2:]\n",
    "    _attentions = [att.detach().numpy() for att in attentions]\n",
    "    attentions_mat = np.asarray(_attentions)[:,0]\n",
    "\n",
    "    all_examples_attentions.append(attentions_mat)\n",
    "    \n",
    "    # Repeating examples and replacing one token at a time with unk\n",
    "    batch_size = 1\n",
    "    max_len = input_ids.shape[1]\n",
    "    \n",
    "    # Repeat each example 'max_len' times\n",
    "    x = input_ids\n",
    "    extended_x = np.reshape(np.tile(x[:,None,...], (1, max_len, 1)),(-1,x.shape[-1]))\n",
    "    #extended_y = np.reshape(np.tile(y[:,None],(1,max_len)),(-1,))\n",
    "    #extened_correct_main_probs = np.reshape(np.tile(correct_main_probs[:,None],(1,max_len)),(-1,))\n",
    "    \n",
    "    # Create unk sequences and unk mask\n",
    "    unktoken = tokenizer.encode([tokenizer.mask_token])[1]\n",
    "    unks = unktoken * np.eye(max_len)\n",
    "    unks =  np.tile(unks, (batch_size, 1))\n",
    "    \n",
    "    unk_mask =  (unktoken - unks)/unktoken\n",
    "  \n",
    "    # Replace one token in each repeatition with unk\n",
    "    extended_x = extended_x * unk_mask + unks\n",
    "    \n",
    "    # Get the new output\n",
    "    extended_predictions = model(torch.tensor(extended_x, dtype=torch.int64))\n",
    "    extended_logits = extended_predictions[0]\n",
    "    extended_probs = torch.nn.Softmax(dim=-1)(extended_logits)\n",
    "    \n",
    "    extended_correct_probs = extended_probs[:,verb_position,actual_verb_index]\n",
    "    extended_wrong_probs =  extended_probs[:,verb_position,inflected_verb_index]\n",
    "    extended_diff_scores = extended_correct_probs - extended_wrong_probs\n",
    "    \n",
    "    # Save the difference in the probability predicted for the correct class\n",
    "    diffs = abs(main_diff_score - extended_diff_scores)\n",
    "\n",
    "    all_examples_blankout_relevance.append(diffs.detach())\n",
    "    n_batches -= 1\n",
    "    if n_batches <= 0:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_examples_raw_relevance[0][0])\n",
    "print(all_examples_joint_relevance[0][0])\n",
    "print(all_examples_blankout_relevance[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examples_attentions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "all_examples_raw_relevance = {}\n",
    "for l in np.arange(6):\n",
    "    all_examples_raw_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.decode(all_examples_x[i][0].numpy())\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_raw_att_relevance(all_examples_attentions[i][...,:length, :length], tokens, layer=l)\n",
    "        all_examples_raw_relevance[l].append(np.asarray(attention_relevance))\n",
    "\n",
    "\n",
    "all_examples_joint_relevance = {}\n",
    "for l in np.arange(6):\n",
    "    all_examples_joint_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.decode(all_examples_x[i][0].numpy())\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_joint_relevance(all_examples_attentions[i][...,:length, :length], tokens, layer=l)\n",
    "        all_examples_joint_relevance[l].append(np.asarray(attention_relevance))\n",
    "    \n",
    "\n",
    "# all_examples_flow_relevance = {}\n",
    "# for l in np.arange(6):\n",
    "#     all_examples_flow_relevance[l] = []\n",
    "#     for i in tqdm(np.arange(len(all_examples_x))):\n",
    "#         tokens = tokenizer.decode(all_examples_x[i][0].numpy())\n",
    "#         length = len(tokens)\n",
    "#         attention_relevance = get_flow_relevance(all_examples_attentions[i][...,:length, :length], tokens, layer=l)\n",
    "#         all_examples_flow_relevance[l].append(np.asarray(attention_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# print(np.mean([spearmanr(all_examples_flow_relevance[i], all_examples_joint_relevance[i]) for i in np.arange(len(all_examples_x))]))\n",
    "# print(np.mean([spearmanr(all_examples_flow_relevance[i], all_examples_blankout_relevance[i]) for i in np.arange(len(all_examples_x))]))\n",
    "\n",
    "for l in np.arange(6):\n",
    "    print(\"layer \",l)\n",
    "    print(all_examples_blankout_relevance[l][0].shape, all_examples_raw_relevance[l][0].shape, all_examples_joint_relevance[l][0].shape)\n",
    "    print('raw:',np.mean([spearmanr(all_examples_raw_relevance[l][i], all_examples_blankout_relevance[i]) for i in np.arange(len(all_examples_x))]))\n",
    "    print('joint',np.mean([spearmanr(all_examples_joint_relevance[l][i], all_examples_blankout_relevance[i]) for i in np.arange(len(all_examples_x))]))\n",
    "    #print('flow',np.mean([spearmanr(all_examples_flow_relevance[l][i], all_examples_blankout_relevance[l][i]) for i in np.arange(len(all_examples_x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearmanr(x, y):\n",
    "    \"\"\" `x`, `y` --> pd.Series\"\"\"\n",
    "    x = pd.Series(x)\n",
    "    y = pd.Series(y)\n",
    "    assert x.shape == y.shape\n",
    "    rx = x.rank(method='dense')\n",
    "    ry = y.rank(method='dense')\n",
    "    d = rx - ry\n",
    "    dsq = np.sum(np.square(d))\n",
    "    n = x.shape[0]\n",
    "    coef = 1. - (6. * dsq) / (n * (n**2 - 1.))\n",
    "    return coef\n",
    "\n",
    "def get_raw_att_relevance(full_att_mat, input_tokens, layer=-1):\n",
    "    cls_index = 0\n",
    "    raw_rel = full_att_mat[layer].sum(axis=0)[cls_index]/full_att_mat[layer].sum(axis=0)[cls_index].sum()\n",
    "    \n",
    "    return raw_rel\n",
    "\n",
    "\n",
    "def get_joint_relevance(full_att_mat, input_tokens, layer=-1):\n",
    "    att_sum_heads =  full_att_mat.sum(axis=1)/8\n",
    "    joint_attentions = compute_joint_attention(att_sum_heads, add_residual=True)\n",
    "    relevance_attentions = joint_attentions[layer][0]\n",
    "    return relevance_attentions\n",
    "\n",
    "\n",
    "def get_flow_relevance(full_att_mat, input_tokens, layer):\n",
    "    \n",
    "    input_tokens = input_tokens\n",
    "    res_att_mat = full_att_mat.sum(axis=1)/full_att_mat.shape[1]\n",
    "    res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...]\n",
    "    res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None]\n",
    "\n",
    "    res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=input_tokens)\n",
    "    \n",
    "    A = res_adj_mat\n",
    "    res_G=nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n",
    "    for i in np.arange(A.shape[0]):\n",
    "        for j in np.arange(A.shape[1]):\n",
    "            nx.set_edge_attributes(res_G, {(i,j): A[i,j]}, 'capacity')\n",
    "\n",
    "\n",
    "    output_nodes = []\n",
    "    input_nodes = []\n",
    "    for key in res_labels_to_index:\n",
    "        if 'L'+str(layer+1) in key:\n",
    "            output_nodes.append(key)\n",
    "        if res_labels_to_index[key] < full_att_mat.shape[-1]:\n",
    "            input_nodes.append(key)\n",
    "    \n",
    "    flow_values = compute_node_flow(res_G, res_labels_to_index, input_nodes, output_nodes, length=full_att_mat.shape[-1])\n",
    "    \n",
    "    n_layers = full_att_mat.shape[0]\n",
    "    length = full_att_mat.shape[-1]\n",
    "    final_layer_attention = flow_values[(layer+1)*length:,layer*length:(layer+1)*length]\n",
    "    cls_index = 0\n",
    "    relevance_attention_raw = final_layer_attention[cls_index]\n",
    "\n",
    "    return relevance_attention_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
